{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "import azure.storage.blob\n",
    "from azure.storage.blob import BlobServiceClient, BlobSasPermissions, generate_blob_sas,generate_container_sas,ContainerClient\n",
    "from azure.storage.blob import ResourceTypes, AccountSasPermissions, generate_account_sas\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, AnalyzeResult\n",
    "from openai import AzureOpenAI\n",
    "import markdown\n",
    "import tiktoken\n",
    "from langchain.text_splitter import TextSplitter, MarkdownTextSplitter, RecursiveCharacterTextSplitter, PythonCodeTextSplitter\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import pyodbc\n",
    "\n",
    "\n",
    "# SharePoint site URL\n",
    "# site_url = \"https://your-sharepoint-site-url\"\n",
    "\n",
    "\n",
    "# The SharePoint folder whose files you want to list\n",
    "folder_url = \"/Shared Documents\"\n",
    "client_id = \"\"\n",
    "tenant_id = \"\"\n",
    "client_secret = \"\"\n",
    "\n",
    "def read_env_file(file_path):\n",
    "    # Load the .env file\n",
    "    load_dotenv(dotenv_path=file_path)\n",
    "\n",
    "    # Get all environment variables\n",
    "    env_vars = os.environ\n",
    "\n",
    "    # Return the environment variables\n",
    "    return env_vars\n",
    "\n",
    "read_env_file(\"secrets.env\")\n",
    "\n",
    "AFR_ENDPOINT = os.environ.get(\"AFR_ENDPOINT\")\n",
    "AFR_API_KEY = os.environ.get(\"AFR_API_KEY\")\n",
    "AZURE_ACC_NAME = os.environ.get(\"AZURE_ACC_NAME\")\n",
    "\n",
    "AZURE_PRIMARY_KEY = os.environ.get(\"AZURE_PRIMARY_KEY\")\n",
    "STORAGE_ACCOUNT_CONTAINER = os.environ.get(\"STORAGE_ACCOUNT_CONTAINER\")\n",
    "DESTINATION_ACCOUNT_CONTAINER = os.environ.get(\"DESTINATION_ACCOUNT_CONTAINER\")\n",
    "\n",
    "OPENAI_ENDPOINT = os.environ.get(\"OPENAI_ENDPOINT\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_EMBEDDING_MODEL = os.environ.get(\"OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "\n",
    "SQL_PASSWORD = os.environ.get(\"SQL_SECRET\")\n",
    "SQL_SERVER = os.environ.get(\"SQL_SERVER\")\n",
    "SQL_DB = os.environ.get(\"SQL_DB\")\n",
    "SQL_USERNAME = os.environ.get(\"SQL_USERNAME\")\n",
    "\n",
    "TEXT_ANALYTICS_KEY = os.environ.get(\"TEXT_ANALYTICS_KEY\")\n",
    "TEXT_ANALYTICS_ENDPOINT = os.environ.get(\"TEXT_ANALYTICS_ENDPOINT\")\n",
    "\n",
    "driver = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "site_id = ''  # replace with your actual site id\n",
    "drive_id = ''  # replace with your actual drive id\n",
    "SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
    "WORDS_BREAKS = list(reversed([\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \"\\n\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reads all the files in a container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#read files from Azure storage account\n",
    "def read_container_files():     \n",
    "     files_to_read = []\n",
    "    \n",
    "     storage_account_connection_string = \"DefaultEndpointsProtocol=https;AccountName=\"+AZURE_ACC_NAME+\";AccountKey=\"+AZURE_PRIMARY_KEY+\";EndpointSuffix=core.windows.net\"\n",
    "    \n",
    "     try:\n",
    "        blob_service_client   = BlobServiceClient.from_connection_string(storage_account_connection_string)   \n",
    "        \n",
    "        sas_token = generate_container_sas(\n",
    "            blob_service_client.account_name,\n",
    "            account_key=blob_service_client.credential.account_key,\n",
    "            container_name=STORAGE_ACCOUNT_CONTAINER,\n",
    "            resource_types=ResourceTypes(object=True),\n",
    "            permission=AccountSasPermissions(read=True,list=True),\n",
    "            expiry=datetime.utcnow() + timedelta(hours=2)\n",
    "        )\n",
    "       \n",
    "        this_container_url = \"https://\"+AZURE_ACC_NAME+\".blob.core.windows.net/\"+STORAGE_ACCOUNT_CONTAINER+\"?\"+sas_token\n",
    "        #print(this_container_url)\n",
    "               \n",
    "       \n",
    "        container = ContainerClient.from_container_url(this_container_url)\n",
    "        #print(container)\n",
    "        blob_list = container.list_blobs()   \n",
    "        #print(blob_list)\n",
    "     \n",
    "        #generate SAS token for each file\n",
    "        for blob in blob_list:                          \n",
    "            doit = True            \n",
    "            \n",
    "            if doit == True:\n",
    "                sas_token = generate_blob_sas(\n",
    "                        account_name=AZURE_ACC_NAME,\n",
    "                        container_name=STORAGE_ACCOUNT_CONTAINER,\n",
    "                        blob_name=blob.name,\n",
    "                        account_key=AZURE_PRIMARY_KEY,\n",
    "                        permission=BlobSasPermissions(read=True),\n",
    "                        expiry=datetime.utcnow() + timedelta(hours=2)\n",
    "                    )\n",
    "                \n",
    "                filewithsas=  \"https://\"+AZURE_ACC_NAME+\".blob.core.windows.net/\"+STORAGE_ACCOUNT_CONTAINER+\"/\"+blob.name+\"?\"+sas_token  \n",
    "\n",
    "                filewithsas = '{\"file\":\"' + filewithsas + '\", \"file_name\":\"' + blob.name + '\"}'            \n",
    "                \n",
    "                # if counter == number_of_blobs:\n",
    "                #     fileswithsas = fileswithsas[:-1]\n",
    "            \n",
    "                files_to_read.append(filewithsas)            \n",
    "      \n",
    "        result = '{\"files\":['+ ','.join(files_to_read) +']}' \n",
    "        return result\n",
    "          \n",
    "     except:\n",
    "        exc_tuple = sys.exc_info()        \n",
    "        errors = [ { \"message\": \"Failure during read_storage_account_files e: \" + str(exc_tuple)}]\n",
    "        print(errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def logging(text):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token estimator to figure out how to chunk the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Callable, List, Dict, Optional, Generator, Tuple, Union\n",
    "\n",
    "\n",
    "class TokenEstimator(object):\n",
    "    GPT2_TOKENIZER = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    def estimate_tokens(self, text: Union[str, List]) -> int:\n",
    "\n",
    "        return len(self.GPT2_TOKENIZER.encode(text, allowed_special=\"all\"))\n",
    "\n",
    "    def construct_tokens_with_size(self, tokens: str, numofTokens: int) -> str:\n",
    "        newTokens = self.GPT2_TOKENIZER.decode(\n",
    "            self.GPT2_TOKENIZER.encode(tokens, allowed_special=\"all\")[:numofTokens]\n",
    "        )\n",
    "        return newTokens\n",
    "\n",
    "TOKEN_ESTIMATOR = TokenEstimator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Phrase extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "def key_phrase_extraction(text):\n",
    "\n",
    "    try:\n",
    "\n",
    "        ta_credential = AzureKeyCredential(TEXT_ANALYTICS_KEY)\n",
    "        text_analytics_client = TextAnalyticsClient(\n",
    "             endpoint=TEXT_ANALYTICS_ENDPOINT, \n",
    "             credential=ta_credential)   \n",
    "\n",
    "\n",
    "        documents = [text]\n",
    "\n",
    "        response = text_analytics_client.extract_key_phrases(documents = documents)[0]\n",
    "        return ', '.join(response.key_phrases) \n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "        \n",
    "#x = key_phrase_extraction(\"this is sample text for restaurants and dinners as well as chicken and beef\")\n",
    "#print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function process a file, chuncks it using Azure Document Intelligence, creates the embeddings for the vector database, then calls the function add to table to save it to your sql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import sqlite3\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "#chunk documents and save it to database\n",
    "\n",
    "def process_file(file_URL,filename,chunck_size=1024):       \n",
    "\n",
    "    endpoint = AFR_ENDPOINT\n",
    "    key = AFR_API_KEY\n",
    "\n",
    "    document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "\n",
    "    if(file_URL != \"\"):\n",
    "      print(f\"Analyzing form from URL {file_URL}...\")\n",
    "    \n",
    "      poller = document_intelligence_client.begin_analyze_document(\n",
    "         \"prebuilt-layout\", AnalyzeDocumentRequest(url_source=file_URL)\n",
    "      )    \n",
    "    \n",
    "    result = poller.result()     \n",
    "    process_afr_result(result, filename, file_URL)   \n",
    "\n",
    "\n",
    "\n",
    "def process_afr_result(result, filename,URL, content_chunk_overlap=100):     \n",
    "    #print(f\"Processing {filename } with {len(result.pages)} pages into database...this might take a few minutes depending on number of pages...\")\n",
    "    chunk_id = 1 \n",
    "    for page_idx in range(len(result.pages)):\n",
    "        docs = []\n",
    "        #pageinfo = result.pages[page_idx]\n",
    "        #print(f\"Processing page {page_idx} of {len(result.pages)}...\")\n",
    "        content_chunk = \"\"       \n",
    "        \n",
    "        for line_idx, line in enumerate(result.pages[page_idx].lines):            \n",
    "            #print(\"...Line # {} has text content '{}'\".format(line_idx,line.content.encode(\"utf-8\")))\n",
    "            # Assuming `line.content` is your text\n",
    "            encoded_content = line.content.encode(\"utf-8\")  # This will give you bytes\n",
    "            decoded_content = encoded_content.decode(\"utf-8\")  # This will give you string\n",
    "            # Now you can add it to your content_chunk\n",
    "            content_chunk += decoded_content + \"\\n\"\n",
    "\n",
    "            \n",
    "             \n",
    "        #now split the chunk        \n",
    "        content_chunk_size=TOKEN_ESTIMATOR.estimate_tokens(content_chunk)\n",
    "        content_chunk_size = 1024;\n",
    "        if content_chunk_size > content_chunk_overlap:\n",
    "           chunk_list = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                                separators=SENTENCE_ENDINGS + WORDS_BREAKS,\n",
    "                                chunk_size=content_chunk_size, chunk_overlap=content_chunk_overlap).split_text(content_chunk)\n",
    "        else:\n",
    "            chunk_list = [content_chunk]\n",
    "        \n",
    "        for chunked_content in chunk_list:\n",
    "            chunk_size = TOKEN_ESTIMATOR.estimate_tokens(chunked_content)\n",
    "            #print(f\"content {chunked_content} with size {chunk_size}\")            \n",
    "            embeddings = create_embeddings(chunked_content)\n",
    "            key_phrases = key_phrase_extraction(chunked_content)\n",
    "            clean_content = chunked_content.replace(\"'\", \"''\")\n",
    "            #sql_statement = f\"exec dbo.InsertIntoDocuments '{filename}', '{URL}', {chunk_id}, '{clean_content}', 'title...', '{datetime.now()}', '{page_idx}', '{line_idx}', '{embeddings}'\"\n",
    "            #print(sql_statement)\n",
    "            add_document_to_table(filename, URL, chunk_id, chunked_content, datetime.now(),  page_idx, line_idx, embeddings,key_phrases)\n",
    "            chunk_id += 1\n",
    "            # if chunk_id != 1:\n",
    "            #    break\n",
    "\n",
    "        # if chunk_id != 1:\n",
    "        #        break    \n",
    "        \n",
    "  \n",
    "def create_embeddings(text): \n",
    "\n",
    "    try:\n",
    "\n",
    "        if text == \"\":\n",
    "            return None\n",
    "        \n",
    "        client = AzureOpenAI(\n",
    "        azure_endpoint = OPENAI_ENDPOINT, \n",
    "        api_key=OPENAI_API_KEY,  \n",
    "        api_version=\"2024-02-15-preview\"\n",
    "        )\n",
    "\n",
    "        return client.embeddings.create(input = [text], model=OPENAI_EMBEDDING_MODEL).data[0].embedding\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error Saving Records for: {e}\")\n",
    " \n",
    "\n",
    "def add_document_to_table(document_name, document_url, chunk_id, content,  last_updated,  page_number, line_number, embeddings,key_phrases):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # Define the connection string                \n",
    "        connection_string = f\"DRIVER={driver};SERVER={SQL_SERVER};DATABASE={SQL_DB};UID={SQL_USERNAME};PWD={SQL_PASSWORD}\"\n",
    "        #print(connection_string)\n",
    "       \n",
    "        # Connect to the database\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        #conn = sqlite3.connect(connection_string)\n",
    "\n",
    "        # Create a new cursor from the connection\n",
    "        cursor = conn.cursor()\n",
    "        clean_content = content.replace(\"'\", \"''\")        \n",
    "        sql_statement = f\"execute dbo.InsertIntoDocuments '{document_name}', '{document_url}', {chunk_id}, '{clean_content}', 'title...', '{last_updated}', '{page_number}', '{line_number}', '{embeddings}', '{key_phrases}'\"\n",
    "        #print(sql_statement)\n",
    "        cursor.execute(sql_statement)       \n",
    "        # Execute the stored procedure        \n",
    "       # cursor.execute(\"{execute dbo.InsertIntoDocuments (?, ?, ?, ?, ?, ?, ?, ?, ?)}\",\n",
    "       #             document_name.replace(\"'\", \" \"), document_url, chunk_id, clean_content, 'title...', last_updated,  page_number, line_number, embeddings)\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "\n",
    "        print(\"Stored procedure executed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error Saving Records for: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "URL = \"https://youraccount.blob.core.windows.net/nasa-files/NEJMoa2404881.pdf?yourSASKEY\"\n",
    "file_name = \"NEJMoa2404881.pdf\"\n",
    "process_file(URL,file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put together the entire thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#now let's read all the darn files\n",
    "files = read_container_files()\n",
    "print(files)\n",
    "\n",
    "data = json.loads(files)\n",
    "\n",
    "for file_info in data['files']:\n",
    "    print('File URL:', file_info['file'])\n",
    "    print('File Name:', file_info['file_name'])\n",
    "    process_file(file_info['file'],file_info['file_name'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
